{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08b5687",
   "metadata": {},
   "source": [
    "# Remote Job Market Intelligence using Ethical Web Scraping\n",
    "\n",
    "**Internship Mini-Project**\n",
    "\n",
    "This notebook documents the entire process of collecting, cleaning, analyzing, and visualizing data from the remote job market, specifically using RemoteOK as the source. The project adheres strictly to ethical scraping guidelines.\n",
    "\n",
    "## 1. Project Setup and Dependencies\n",
    "The following libraries are required for this project:\n",
    "```bash\n",
    "pip install requests beautifulsoup4 pandas matplotlib seaborn\n",
    "```\n",
    "\n",
    "The project structure is organized for clarity and reproducibility:\n",
    "```\n",
    "remoteok_project/\n",
    "├── data/\n",
    "├── scripts/\n",
    "├── visualizations/\n",
    "├── REPORT.md\n",
    "└── remoteok_scraping_project.ipynb (This file)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549700f",
   "metadata": {},
   "source": [
    "## 2. Ethical Web Scraping Implementation\n",
    "\n",
    "The core of the data collection is the `RemoteOKScraper` class. It uses the `requests` library to fetch the page and `BeautifulSoup` to parse the job listings. Crucially, it is designed to be ethical by setting a proper User-Agent and respecting the site's `robots.txt` rules (e.g., implementing a crawl delay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0783a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to https://remoteok.com/...\n",
      "Found 9 potential job listings.\n",
      "Data saved to C:/Users/ashis/Downloads/Evoastra_MiniProject/remoteok_internship_final/remoteok_project/data/remoteok_jobs_raw.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "class RemoteOKScraper:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://remoteok.com/\"\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
    "        }\n",
    "        self.jobs_data = []\n",
    "\n",
    "    def fetch_page(self):\n",
    "        \"\"\"Fetches the main page of RemoteOK.\"\"\"\n",
    "        print(f\"Connecting to {self.url}...\")\n",
    "        try:\n",
    "            # In a real scenario, we hadd use a session and handle retries\n",
    "            response = requests.get(self.url, headers=self.headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            return response.content\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching page: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_jobs(self, html_content):\n",
    "        \"\"\"Parses job listings from the HTML content.\"\"\"\n",
    "        if not html_content:\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        job_rows = soup.find_all('tr', class_='job')\n",
    "        \n",
    "        print(f\"Found {len(job_rows)} potential job listings.\")\n",
    "\n",
    "        for row in job_rows:\n",
    "            try:\n",
    "                # Extracting data based on RemoteOK's structure\n",
    "                title = row.find('h2', itemprop='title').text.strip() if row.find('h2', itemprop='title') else \"N/A\"\n",
    "                company = row.find('h3', itemprop='name').text.strip() if row.find('h3', itemprop='name') else \"N/A\"\n",
    "                \n",
    "                # Tags/Skills\n",
    "                tags = [tag.text.strip() for tag in row.find_all('div', class_='tag')]\n",
    "                skills = \", \".join(tags)\n",
    "                locations = [loc.text.strip() for loc in row.find_all('div', class_='location')]\n",
    "                \n",
    "                # Usually, the first location div is the actual location, \n",
    "                # and others might be salary or job type\n",
    "                location = \"Remote\"\n",
    "                salary = \"N/A\"\n",
    "                job_type = \"Full-Time\" # Default\n",
    "                \n",
    "                for loc in locations:\n",
    "                    if '$' in loc:\n",
    "                        salary = loc\n",
    "                    elif any(t in loc for t in ['Full-time', 'Contract', 'Part-time']):\n",
    "                        job_type = loc\n",
    "                    else:\n",
    "                        location = loc\n",
    "\n",
    "                # Date posted (RemoteOK uses 'time' tag)\n",
    "                date_elem = row.find('time')\n",
    "                date_posted = date_elem['datetime'] if date_elem and date_elem.has_attr('datetime') else \"N/A\"\n",
    "\n",
    "                self.jobs_data.append({\n",
    "                    'title': title,\n",
    "                    'company': company,\n",
    "                    'skills': skills,\n",
    "                    'location': location,\n",
    "                    'job_type': job_type,\n",
    "                    'salary': salary,\n",
    "                    'date_posted': date_posted\n",
    "                })\n",
    "                \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing a job row: {e}\")\n",
    "                continue\n",
    "\n",
    "    def save_to_csv(self, filename=\"remoteok_jobs.csv\"):\n",
    "        \"\"\"Saves the collected data to a CSV file.\"\"\"\n",
    "        if not self.jobs_data:\n",
    "            print(\"No data to save.\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.jobs_data)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main execution method.\"\"\"\n",
    "        content = self.fetch_page()\n",
    "        if content:\n",
    "            self.parse_jobs(content)\n",
    "            self.save_to_csv(\"C:/Users/ashis/Downloads/Evoastra_MiniProject/remoteok_internship_final/remoteok_project/data/remoteok_jobs_raw.csv\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve data. Please check connection or site status.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = RemoteOKScraper()\n",
    "    scraper.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf8f916",
   "metadata": {},
   "source": [
    "## 3. Data Analysis and Visualization\n",
    "\n",
    "The analysis script processes the cleaned data (`remoteok_jobs_cleaned.csv`) to generate key market intelligence insights. We use `pandas` for data manipulation and `matplotlib`/`seaborn` for professional visualizations. The visualizations are saved to the `visualizations/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783486f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BackendFilter' from 'matplotlib.backends' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Analysis Code (scripts/analysis.py)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Removed the try-except block for setting the backend as it is unnecessary in Jupyter Notebook\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:159\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\matplotlib\\rcsetup.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BackendFilter' from 'matplotlib.backends' (unknown location)"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Removed the try-except block for setting the backend as it is unnecessary in Jupyter Notebook\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# sstyle for visualizations\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "def run_analysis():\n",
    "    # Load the data\n",
    "    data_path = './data/remoteok_jobs_cleaned.csv' # Adjusted path for notebook context\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Error: {data_path} not found.\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    os.makedirs(\"./visualizations\", exist_ok=True)\n",
    "\n",
    "    # --- Visualization 1: Top 10 Skills Demand (Bar Chart) ---\n",
    "    print(\"Generating Visualization 1: Top 10 Skills...\")\n",
    "    df_skills = df.copy()\n",
    "    df_skills['skills'] = df_skills['skills'].str.split(', ')\n",
    "    df_skills = df_skills.explode('skills')\n",
    "    top_skills = df_skills['skills'].value_counts().head(10)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=top_skills.index, y=top_skills.values, palette=\"viridis\")\n",
    "    plt.title('Top 10 Most Demanded Skills in Remote Jobs', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Skill', fontsize=12)\n",
    "    plt.ylabel('Number of Job Postings', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./visualizations/top_skills.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # --- Visualization 2: Job Type Distribution (Pie Chart) ---\n",
    "    print(\"Generating Visualization 2: Job Type Distribution...\")\n",
    "    job_type_counts = df['job_type'].value_counts()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.pie(job_type_counts, labels=job_type_counts.index, autopct='%1.1f%%', startangle=90, colors=sns.color_palette(\"pastel\"))\n",
    "    plt.title('Distribution of Job Types in Remote Jobs', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./visualizations/job_type_distribution.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # --- Visualization 3: Top 10 Job Titles (Horizontal Bar Chart) ---\n",
    "    print(\"Generating Visualization 3: Top 10 Job Titles...\")\n",
    "    top_titles = df['title'].value_counts().head(10)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=top_titles.values, y=top_titles.index, palette=\"magma\")\n",
    "    plt.title('Top 10 Most Common Remote Job Titles', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Number of Postings', fontsize=12)\n",
    "    plt.ylabel('Job Title', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./visualizations/top_job_titles.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # --- Visualization 4: Skill Frequency Comparison (Horizontal Bar Chart) ---\n",
    "    print(\"Generating Visualization 4: Skill Frequency Comparison...\")\n",
    "    top_skills_extended = df_skills['skills'].value_counts().head(15)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(x=top_skills_extended.values, y=top_skills_extended.index, palette=\"coolwarm\")\n",
    "    plt.title('Top 15 Skills Frequency in Remote Job Postings', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Frequency (Count)', fontsize=12)\n",
    "    plt.ylabel('Skill', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./visualizations/skill_frequency_comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # --- Comparative Analysis 1: Contract vs Full-Time Roles ---\n",
    "    print(\"\\n--- Comparative Analysis: Contract vs Full-Time ---\")\n",
    "    full_time = df[df['job_type'] == 'Full-Time'].copy()\n",
    "    contract = df[df['job_type'] == 'Contract'].copy()\n",
    "\n",
    "    def get_top_skills(subset_df, n=10):\n",
    "        s = subset_df.copy()\n",
    "        s['skills'] = s['skills'].str.split(', ')\n",
    "        s = s.explode('skills')\n",
    "        return s['skills'].value_counts().head(n)\n",
    "\n",
    "    top_ft_skills = get_top_skills(full_time)\n",
    "    top_c_skills = get_top_skills(contract)\n",
    "\n",
    "    print(\"Top Skills for Full-Time Jobs:\")\n",
    "    print(top_ft_skills)\n",
    "    print(\"\\nTop Skills for Contract Jobs:\")\n",
    "    print(top_c_skills)\n",
    "\n",
    "    # --- Comparative Analysis 2: Skill Demand Across Job Titles ---\n",
    "    print(\"\\n--- Comparative Analysis: Skill Demand Across Top Titles ---\")\n",
    "    top_3_titles = df['title'].value_counts().head(3).index.tolist()\n",
    "\n",
    "    for title in top_3_titles:\n",
    "        title_jobs = df[df['title'] == title].copy()\n",
    "        title_skills = get_top_skills(title_jobs, 5)\n",
    "        print(f\"\\nTop skills for '{title}':\")\n",
    "        print(title_skills)\n",
    "\n",
    "    print(\"\\nAnalysis complete. Visualizations saved in './visualizations/'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f35fe",
   "metadata": {},
   "source": [
    "## 4. Project Report and Conclusion\n",
    "The visualizations generated by the code above are available in the `visualizations/` folder.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
